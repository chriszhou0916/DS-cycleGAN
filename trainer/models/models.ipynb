{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class MyInstanceNorm(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 axis=-1,\n",
    "                 epsilon=1e-3,\n",
    "                 center=True,\n",
    "                 scale=True,\n",
    "                 beta_initializer='zeros',\n",
    "                 gamma_initializer='ones',\n",
    "                 beta_regularizer=None,\n",
    "                 gamma_regularizer=None,\n",
    "                 beta_constraint=None,\n",
    "                 gamma_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(MyInstanceNorm, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.axis = axis\n",
    "        self.epsilon = epsilon\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "        self.beta_initializer = tf.keras.initializers.get(beta_initializer)\n",
    "        self.gamma_initializer = tf.keras.initializers.get(gamma_initializer)\n",
    "        self.beta_regularizer = tf.keras.regularizers.get(beta_regularizer)\n",
    "        self.gamma_regularizer = tf.keras.regularizers.get(gamma_regularizer)\n",
    "        self.beta_constraint = tf.keras.constraints.get(beta_constraint)\n",
    "        self.gamma_constraint = tf.keras.constraints.get(gamma_constraint)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self._add_gamma_weight(input_shape)\n",
    "        self._add_beta_weight(input_shape)\n",
    "        self.built = True\n",
    "        super(MyInstanceNorm, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.keras.backend.int_shape(inputs)\n",
    "        tensor_input_shape = tf.shape(inputs)\n",
    "        mean, variance = tf.nn.moments(inputs, [1, 2], keepdims=True)\n",
    "        weight_shape = self._create_broadcast_shape(input_shape)\n",
    "        expanded_beta, expanded_gamma = self._get_reshaped_weights(input_shape, weight_shape, broadcast=False)\n",
    "        outputs = tf.nn.batch_normalization(inputs, mean, variance, offset=expanded_beta, scale=expanded_gamma,\n",
    "                                            variance_epsilon=self.epsilon)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def _get_reshaped_weights(self, input_shape, weight_shape, broadcast=False):\n",
    "        gamma = None\n",
    "        beta = None\n",
    "        if self.scale:\n",
    "            gamma = tf.reshape(self.gamma, weight_shape)\n",
    "        if self.center:\n",
    "            beta = tf.reshape(self.beta, weight_shape)\n",
    "        return gamma, beta\n",
    "    \n",
    "    def _create_broadcast_shape(self, input_shape):\n",
    "        broadcast_shape = [1] * (len(input_shape) - 1)\n",
    "        broadcast_shape[self.axis] = input_shape[self.axis]\n",
    "        return broadcast_shape\n",
    "    \n",
    "    def _add_gamma_weight(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        shape = (dim,)\n",
    "\n",
    "        if self.scale:\n",
    "            self.gamma = self.add_weight(\n",
    "                shape=shape,\n",
    "                name='gamma',\n",
    "                initializer=self.gamma_initializer,\n",
    "                regularizer=self.gamma_regularizer,\n",
    "                constraint=self.gamma_constraint)\n",
    "        else:\n",
    "            self.gamma = None\n",
    "    def _add_beta_weight(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        shape = (dim,)\n",
    "\n",
    "        if self.center:\n",
    "            self.beta = self.add_weight(\n",
    "                shape=shape,\n",
    "                name='beta',\n",
    "                initializer=self.beta_initializer,\n",
    "                regularizer=self.beta_regularizer,\n",
    "                constraint=self.beta_constraint)\n",
    "        else:\n",
    "            self.beta = None\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'axis':\n",
    "            self.axis,\n",
    "            'epsilon':\n",
    "            self.epsilon,\n",
    "            'center':\n",
    "            self.center,\n",
    "            'scale':\n",
    "            self.scale,\n",
    "            'beta_initializer':\n",
    "            tf.keras.initializers.serialize(self.beta_initializer),\n",
    "            'gamma_initializer':\n",
    "            tf.keras.initializers.serialize(self.gamma_initializer),\n",
    "            'beta_regularizer':\n",
    "            tf.keras.regularizers.serialize(self.beta_regularizer),\n",
    "            'gamma_regularizer':\n",
    "            tf.keras.regularizers.serialize(self.gamma_regularizer),\n",
    "            'beta_constraint':\n",
    "            tf.keras.constraints.serialize(self.beta_constraint),\n",
    "            'gamma_constraint':\n",
    "            tf.keras.constraints.serialize(self.gamma_constraint)\n",
    "        }\n",
    "        base_config = super(MyInstanceNorm, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "\n",
    "class ReflectionPadding2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [tf.keras.layers.InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'padding':\n",
    "            self.padding\n",
    "        }\n",
    "        base_config = super(ReflectionPadding2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "def normalization(intput_tensor, method='instance'):\n",
    "  if method == 'instance':\n",
    "    x = MyInstanceNorm(center=True, scale=True,\n",
    "                                                  beta_initializer=\"random_uniform\",\n",
    "                                                  gamma_initializer=\"random_uniform\")(intput_tensor)\n",
    "  else:\n",
    "    x = tf.keras.layers.BatchNormalization()(intput_tensor)\n",
    "  return x\n",
    "\n",
    "def conv_w_reflection(input_tensor,\n",
    "               kernel_size,\n",
    "               filters,\n",
    "               stride):\n",
    "  p = kernel_size // 2\n",
    "  x = ReflectionPadding2D(padding=(p, p))(input_tensor)\n",
    "  x = tf.keras.layers.Conv2D(filters, kernel_size, strides=stride, use_bias=True)(x)\n",
    "  x = normalization(x, method='batch')\n",
    "  x = tf.keras.layers.Activation(tf.nn.relu)(x)\n",
    "  return x\n",
    "\n",
    "def conv_block(input_tensor, filters):\n",
    "  x = ReflectionPadding2D(padding=(1, 1))(input_tensor)\n",
    "  x = tf.keras.layers.Conv2D(filters, kernel_size=3, strides=(1, 1), use_bias=True)(x)\n",
    "  x = normalization(x, method='batch')\n",
    "  x = tf.keras.layers.Activation(tf.nn.relu)(x)\n",
    "\n",
    "  x = ReflectionPadding2D(padding=(1, 1))(x)\n",
    "  x = tf.keras.layers.Conv2D(filters, kernel_size=3, strides=(1, 1), use_bias=True)(x)\n",
    "  x = normalization(x, method='batch')\n",
    "  return x\n",
    "\n",
    "def residual_block(input_tensor, filters):\n",
    "  b1 = conv_block(input_tensor, filters)\n",
    "  x = tf.keras.layers.Add()([input_tensor, b1])\n",
    "  return x\n",
    "\n",
    "def upsample_conv(input_tensor, kernel_size, filters, stride):\n",
    "  x = tf.keras.layers.Conv2DTranspose(filters, kernel_size, strides=stride, padding='same', use_bias=True)(input_tensor)\n",
    "  x = normalization(x, method='batch')\n",
    "  x = tf.keras.layers.Activation(tf.nn.relu)(x)\n",
    "  return x\n",
    "\n",
    "def create_generator(shape=(256, 256, 3)):\n",
    "    inputs = tf.keras.layers.Input(shape=shape)\n",
    "    x = conv_w_reflection(inputs, 7, 64, 1)\n",
    "    x = conv_w_reflection(x, 3, 128, 2)\n",
    "    x = conv_w_reflection(x, 3, 256, 2)\n",
    "    x = residual_block(x, 256)\n",
    "    x = residual_block(x, 256)\n",
    "    x = residual_block(x, 256)\n",
    "\n",
    "    x = residual_block(x, 256)\n",
    "    x = residual_block(x, 256)\n",
    "    x = residual_block(x, 256)\n",
    "\n",
    "    x = residual_block(x, 256)\n",
    "    x = residual_block(x, 256)\n",
    "    x = residual_block(x, 256)\n",
    "    x = upsample_conv(x, 3, 128, 2)\n",
    "    x = upsample_conv(x, 3, 64, 2)\n",
    "    x = ReflectionPadding2D(padding=(3, 3))(x)\n",
    "    x = tf.keras.layers.Conv2D(3, 7, strides=1, activation='tanh')(x)\n",
    "#     x = tf.keras.layers.Conv2DTranspose(output_dim, kernel_size=7, strides=1, padding='same', activation='tanh')(x)\n",
    "    x = tf.keras.layers.Lambda(lambda x: tf.math.scalar_mul(.5, x) + .5)(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "#     return x\n",
    "\n",
    "def unet_downsample(input_tensor, filters, size, apply_norm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    x = tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                                    kernel_initializer=initializer, use_bias=True)(input_tensor)\n",
    "    if apply_norm:\n",
    "        x = normalization(x, method='instance')\n",
    "    x = tf.keras.layers.Activation(tf.nn.leaky_relu)(x)\n",
    "    return x\n",
    "\n",
    "def unet_upsample(input_tensor, filters, size, apply_dropout=False, last=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    x = tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding='same',\n",
    "                                    kernel_initializer=initializer, use_bias=True)(input_tensor)\n",
    "    x = normalization(x, method='instance')\n",
    "    if apply_dropout:\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    if last:\n",
    "        x = tf.keras.layers.Activation(tf.nn.tanh)(x)\n",
    "    else:\n",
    "        x = tf.keras.layers.Activation(tf.nn.relu)(x)\n",
    "    return x\n",
    "\n",
    "def create_unet_generator(shape=(256, 256, 3)):\n",
    "    inputs = tf.keras.layers.Input(shape=shape)\n",
    "    down1 = unet_downsample(inputs, 64, 4, apply_norm=False) # 128,128,64\n",
    "    down2 = unet_downsample(down1, 128, 4) # 64,64,128\n",
    "    down3 = unet_downsample(down2, 256, 4) # 32,32,256\n",
    "    down4 = unet_downsample(down3, 512, 4) # 16,16,512\n",
    "    down5 = unet_downsample(down4, 512, 4) # 8,8,512\n",
    "    down6 = unet_downsample(down5, 512, 4) # 4,4,512\n",
    "    down7 = unet_downsample(down6, 512, 4) # 2,2,512\n",
    "    down8 = unet_downsample(down7, 512, 4) # 1,1,512\n",
    "    up1 = unet_upsample(down8, 512, 4, apply_dropout=True) # 2,2,512\n",
    "    up1 = tf.keras.layers.Concatenate()([up1, down7]) # 2,2,1024\n",
    "    up2 = unet_upsample(up1, 512, 4, apply_dropout=True) # 4,4,512\n",
    "    up2 = tf.keras.layers.Concatenate()([up2, down6]) # 4,4,1024\n",
    "    up3 = unet_upsample(up2, 512, 4, apply_dropout=True) # 8,8,512\n",
    "    up3 = tf.keras.layers.Concatenate()([up3, down5]) # 8,8,1024\n",
    "    up4 = unet_upsample(up3, 512, 4) # 16,16,512\n",
    "    up4 = tf.keras.layers.Concatenate()([up4, down4]) # 16,16,1024\n",
    "    up5 = unet_upsample(up4, 256, 4) # 32,32,256\n",
    "    up5 = tf.keras.layers.Concatenate()([up5, down3]) # 32,32,512\n",
    "    up6 = unet_upsample(up5, 128, 4) # 64,64,128\n",
    "    up6 = tf.keras.layers.Concatenate()([up6, down2]) # 64,64,256\n",
    "    up7 = unet_upsample(up6, 64, 4) # 128,128,64\n",
    "    up7 = tf.keras.layers.Concatenate()([up7, down1]) # 128,128,128\n",
    "    up8 = unet_upsample(up7, 3, 4, last=True) # 256,256,3\n",
    "    x = tf.keras.layers.Lambda(lambda x: tf.math.scalar_mul(.5, x) + .5)(up8)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "def dis_downsample(input_tensor,\n",
    "               kernel_size,\n",
    "               filters,\n",
    "               stride, norm=None):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "  p = 1\n",
    "  x = ReflectionPadding2D(padding=(p, p))(input_tensor)\n",
    "  x = tf.keras.layers.Conv2D(filters, kernel_size, strides=stride, kernel_initializer=initializer)(x)\n",
    "  if norm is not None:\n",
    "    x = normalization(x, method=norm)\n",
    "  x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "  return x\n",
    "\n",
    "def create_discriminator(shape=(256, 256, 3)):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    inputs = tf.keras.layers.Input(shape=shape)\n",
    "    x = dis_downsample(inputs, 4, 64, 2, norm=None)\n",
    "    x = dis_downsample(x, 4, 128, 2, norm='batch')\n",
    "    x = dis_downsample(x, 4, 256, 2, norm='batch')\n",
    "    x = dis_downsample(x, 4, 512, 1, norm='batch')\n",
    "    x = ReflectionPadding2D(padding=(1, 1))(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=1, kernel_size=4, strides=1, kernel_initializer=initializer)(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "def create_LSdiscriminator(shape=(256, 256, 3)):\n",
    "    inputs = tf.keras.layers.Input(shape=shape)\n",
    "    x = dis_downsample(inputs, 5, 64, 2, norm=None)\n",
    "    x = dis_downsample(x, 5, 128, 2, norm='instance')\n",
    "    x = dis_downsample(x, 5, 256, 2, norm='instance')\n",
    "    x = dis_downsample(x, 5, 512, 2, norm='instance')\n",
    "    x = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_52 (Reflec (None, 262, 262, 3)  0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 256, 256, 64) 9472        reflection_padding2d_52[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 256, 256, 64) 256         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 256, 256, 64) 0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_53 (Reflec (None, 258, 258, 64) 0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 128, 128, 128 73856       reflection_padding2d_53[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 128, 128, 128 512         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 128, 128, 128 0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_54 (Reflec (None, 130, 130, 128 0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 64, 64, 256)  295168      reflection_padding2d_54[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 64, 64, 256)  1024        conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 64, 64, 256)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_55 (Reflec (None, 66, 66, 256)  0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_55[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 64, 64, 256)  1024        conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 64, 64, 256)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_56 (Reflec (None, 66, 66, 256)  0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_56[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 64, 64, 256)  1024        conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 64, 64, 256)  0           activation_16[0][0]              \n",
      "                                                                 batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_57 (Reflec (None, 66, 66, 256)  0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_57[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 64, 64, 256)  1024        conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 64, 64, 256)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_58 (Reflec (None, 66, 66, 256)  0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_58[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 64, 64, 256)  1024        conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 64, 64, 256)  0           add_9[0][0]                      \n",
      "                                                                 batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_59 (Reflec (None, 66, 66, 256)  0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_59[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 64, 64, 256)  1024        conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 64, 64, 256)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_60 (Reflec (None, 66, 66, 256)  0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_60[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 64, 64, 256)  1024        conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 64, 64, 256)  0           add_10[0][0]                     \n",
      "                                                                 batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_61 (Reflec (None, 66, 66, 256)  0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_61[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 64, 64, 256)  1024        conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 64, 64, 256)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_62 (Reflec (None, 66, 66, 256)  0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_62[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 64, 64, 256)  1024        conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 64, 64, 256)  0           add_11[0][0]                     \n",
      "                                                                 batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_63 (Reflec (None, 66, 66, 256)  0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_63[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 64, 64, 256)  1024        conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 64, 64, 256)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_64 (Reflec (None, 66, 66, 256)  0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_64[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 64, 64, 256)  1024        conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 64, 64, 256)  0           add_12[0][0]                     \n",
      "                                                                 batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_65 (Reflec (None, 66, 66, 256)  0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_65[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 64, 64, 256)  1024        conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 64, 64, 256)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_66 (Reflec (None, 66, 66, 256)  0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_66[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 64, 64, 256)  1024        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 64, 64, 256)  0           add_13[0][0]                     \n",
      "                                                                 batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_67 (Reflec (None, 66, 66, 256)  0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_67[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 64, 64, 256)  1024        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 64, 64, 256)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_68 (Reflec (None, 66, 66, 256)  0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_68[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 64, 64, 256)  1024        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 64, 64, 256)  0           add_14[0][0]                     \n",
      "                                                                 batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_69 (Reflec (None, 66, 66, 256)  0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_69[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 64, 64, 256)  1024        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 64, 64, 256)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_70 (Reflec (None, 66, 66, 256)  0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_70[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 64, 64, 256)  1024        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 64, 64, 256)  0           add_15[0][0]                     \n",
      "                                                                 batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_71 (Reflec (None, 66, 66, 256)  0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_71[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 64, 64, 256)  1024        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 64, 64, 256)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_72 (Reflec (None, 66, 66, 256)  0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_72[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 64, 64, 256)  1024        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 64, 64, 256)  0           add_16[0][0]                     \n",
      "                                                                 batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 128, 128, 128 295040      add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 128, 128, 128 512         conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 128, 128, 128 0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 256, 256, 64) 73792       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 256, 256, 64) 256         conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 256, 256, 64) 0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_73 (Reflec (None, 262, 262, 64) 0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 256, 256, 3)  9411        reflection_padding2d_73[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 256, 256, 3)  0           conv2d_84[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 11,399,171\n",
      "Trainable params: 11,388,675\n",
      "Non-trainable params: 10,496\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "g = create_generator()\n",
    "g.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
